{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“œ Historical Handwritten Language Translator (HTR + NLP)\n",
        "\n",
        "### âœ… What this notebook demonstrates\n",
        "This notebook builds a complete pipeline for historical manuscript digitization:\n",
        "\n",
        "**Image Upload â†’ Handwriting Transcription (Qwen2.5-VL) â†’ Text Refinement + Modern Translation (Groq LLaMA) â†’ Streamlit Web App â†’ Public Link via Cloudflare Tunnel**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© Key components\n",
        "- **Qwen2.5-VL (Vision-Language Model)**: reads handwritten manuscripts from images (Chinese / Old English)\n",
        "- **Groq (LLM API inference)**: refines transcription + performs modernization/translation\n",
        "- **Streamlit**: lightweight UI for demo\n",
        "- **Cloudflare Tunnel**: exposes the app publicly without deployment\n",
        "\n",
        "> âš ï¸ Note: This project uses **pretrained models** (no training dataset required). It runs in **inference-only** mode.\n"
      ],
      "metadata": {
        "id": "WwtnWXNAlTyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Installing Dependencies & Setting Up Environment\n",
        "\n",
        "This section installs all required libraries:\n",
        "- Streamlit for UI\n",
        "- PyTorch + Transformers for model loading\n",
        "- BitsAndBytes for 4-bit quantization\n",
        "- Groq SDK for LLM API calls\n",
        "- Qwen VL utilities for multimodal processing\n"
      ],
      "metadata": {
        "id": "o9hKc8AVlXkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Installing Dependencies & Setup Environment**"
      ],
      "metadata": {
        "id": "ZWjt5536qBf4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ40E41rZs5p",
        "outputId": "3a06fbcd-295e-45a7-b878-c2cb7559faec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… Environment Installed. Ready for Model Setup.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Installing core libraries\n",
        "!pip install -q streamlit pyngrok torch torchvision transformers accelerate\n",
        "!pip install -q opencv-python-headless pillow\n",
        "!pip install -q python-dotenv\n",
        "!pip install -U bitsandbytes>=0.46.1\n",
        "!pip install groq\n",
        "# Install Qwen-VL specific utilities (Critical for Arabic HTR)\n",
        "!pip install -q qwen-vl-utils git+https://github.com/huggingface/transformers\n",
        "\n",
        "print(\"âœ… Environment Installed. Ready for Model Setup.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Creating the Streamlit Web Application (`app.py`)\n",
        "\n",
        "This cell writes the full Streamlit app into a Python file.\n",
        "The app contains:\n",
        "\n",
        "### Phase 1: HTR (Handwritten Text Recognition)\n",
        "- Uses **Qwen2.5-VL** to transcribe handwriting from the uploaded image.\n",
        "\n",
        "### Phase 2: NLP Refinement + Translation\n",
        "- Uses **Groq API (LLaMA 3.3 70B)** to:\n",
        "  - Correct transcription errors\n",
        "  - Expand abbreviations (Old English)\n",
        "  - Output modern English translation\n"
      ],
      "metadata": {
        "id": "t1M-OJnQlibT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Creating the Streamlit App**"
      ],
      "metadata": {
        "id": "nFN7Dwp_qMFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import os\n",
        "from groq import Groq\n",
        "# --- 0. MEMORY OPTIMIZATION ---\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# --- PAGE CONFIGURATION ---\n",
        "st.set_page_config(page_title=\"Historical Handwritten Language Translator (HTR & NLP)\", layout=\"wide\", page_icon=\"ğŸ“œ\")\n",
        "\n",
        "\n",
        "\n",
        "# --- DEVICE SETUP ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if DEVICE == \"cpu\":\n",
        "    st.sidebar.warning(\"âš ï¸ Running on CPU. Switch to T4 GPU runtime for speed.\")\n",
        "\n",
        "# --- 1. MODEL LOADING (4-BIT QUANTIZED)---\n",
        "@st.cache_resource\n",
        "def load_qwen_model():\n",
        "    print(\"Loading Qwen2.5-VL (with 4-bit Quantization)...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "    return processor, model\n",
        "\n",
        "# --- 2. PROCESSING FUNCTIONS ---\n",
        "\n",
        "def resize_image_if_needed(image, max_size=1280):\n",
        "    width, height = image.size\n",
        "    if width > max_size or height > max_size:\n",
        "        image.thumbnail((max_size, max_size))\n",
        "    return image\n",
        "\n",
        "def extract_text_qwen(image, processor, model, mode):\n",
        "    image = resize_image_if_needed(image)\n",
        "\n",
        "    # --- PROMPTS FOR CHINESE ---\n",
        "    if mode == \"Chinese\":\n",
        "        # Qwen is S-Tier at Chinese; we ask for precise character transcription\n",
        "        prompt_text = \"Transcribe the handwritten Chinese text in this image exactly. Identify specific characters (Traditional or Simplified) as written. Do not add commentary.\"\n",
        "    else:\n",
        "        prompt_text = \"Transcribe this handwritten English manuscript exactly. Maintain line breaks and original spelling.\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt_text},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=1024) # Increased token limit for dense Chinese text\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def process_with_llm(text, mode):\n",
        "    api_key ='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'\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    # Llama 3.3 70B is excellent for reasoning/translation and is free on Groq\n",
        "    model_id = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "    if mode == \"Chinese\":\n",
        "        system_prompt = \"You are an expert translator of Chinese Calligraphy.\"\n",
        "        user_prompt = f\"\"\"\n",
        "        1. Correct any OCR errors in this text: \"{text}\"\n",
        "        2. Translate it into Modern English.\n",
        "\n",
        "        Output format:\n",
        "        **Corrected Chinese:**\n",
        "        [text]\n",
        "        **Modern Translation:**\n",
        "        [text]\n",
        "        \"\"\"\n",
        "    else:\n",
        "        system_prompt = \"You are an expert paleographer specializing in 18th-century manuscripts.\"\n",
        "        user_prompt = f\"\"\"\n",
        "        1. Correct any OCR errors in this text (it is 18th-century English): \"{text}\"\n",
        "        2. Expand abbreviations (e.g., \"&c\" -> \"etc.\").\n",
        "        3. Translate/Modernize it into clear Modern UK English.\n",
        "\n",
        "        Output format:\n",
        "        **Corrected Transcription:**\n",
        "        [text]\n",
        "        **Modern Translation:**\n",
        "        [text]\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.1,  # Lower temperature = more faithful transcription\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Groq API Error: {str(e)}\"\n",
        "\n",
        "# --- 3. MAIN UI LAYOUT ---\n",
        "\n",
        "st.title(\"ğŸ“œ Historical Manuscript Digitizer\")\n",
        "st.markdown(\"**Pipeline:** Image â†’ HTR through **Qwen2.5-VL** (Chinese/English) â†’ and then NMT through an LLM \")\n",
        "\n",
        "col_opt, col_file = st.columns([1, 2])\n",
        "with col_opt:\n",
        "    # UPDATED: Replaced Arabic with Chinese\n",
        "    mode = st.radio(\"Manuscript Language\", [\"Chinese\", \"Old English\"])\n",
        "\n",
        "with col_file:\n",
        "    uploaded_file = st.file_uploader(\"Upload Manuscript\", type=['jpg', 'png', 'jpeg', 'tiff', 'tif'])\n",
        "\n",
        "if uploaded_file and st.button(\"ğŸš€ Digitize & Translate\"):\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "            with col1:\n",
        "                # UPDATED: using use_container_width instead of use_column_width\n",
        "                st.image(image, caption=\"Original Document\", use_container_width=True)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error opening image: {e}\")\n",
        "            st.stop()\n",
        "\n",
        "        # --- PHASE 1: HTR (Local GPU) ---\n",
        "        with st.spinner(f\"Reading {mode} Handwriting (Qwen2.5-VL)...\"):\n",
        "            try:\n",
        "                proc, model = load_qwen_model()\n",
        "                raw_text = extract_text_qwen(image, proc, model, mode)\n",
        "                st.toast(\"HTR Complete!\", icon=\"âœ…\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"HTR Error: {e}\")\n",
        "                st.stop()\n",
        "\n",
        "        # --- PHASE 2: LLM (Cloud API) ---\n",
        "        with st.spinner(\"Refining & Translating (LLM API)...\"):\n",
        "            try:\n",
        "                result = process_with_llm(raw_text, mode)\n",
        "                with col1:\n",
        "                    st.subheader(\"ğŸ“ Extracted Text\")\n",
        "                    st.text_area(\"Raw Output\", raw_text, height=300)\n",
        "                with col2:\n",
        "                    st.subheader(\"ğŸŒ Translation\")\n",
        "                    st.markdown(result)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Translation Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY2ZMqVwntOL",
        "outputId": "e69aebfc-197b-424f-edcf-5d7b38e2d83f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Launching Streamlit App + Cloudflare Tunnel (Public Demo Link)\n",
        "\n",
        "This section:\n",
        "1. Installs Cloudflared\n",
        "2. Runs Streamlit on port **8501**\n",
        "3. Creates a **temporary public URL** using Cloudflare Tunnel\n",
        "4. Prints the live demo link (trycloudflare.com)\n"
      ],
      "metadata": {
        "id": "-Ct8CH-6l4Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Launch App with Cloudflare Tunnel**"
      ],
      "metadata": {
        "id": "66WEsKELrqK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import subprocess\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# 1. Install Cloudflared\n",
        "print(\"Installing Cloudflared...\")\n",
        "!wget -q -O cloudflared-linux-amd64.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1\n",
        "\n",
        "# 2. Run Streamlit in the background\n",
        "print(\"Starting Streamlit App...\")\n",
        "# We redirect output to devnull so it doesn't clutter the console\n",
        "# Updated startup command to disable CORS and XSRF protection\n",
        "subprocess.Popen([\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port=8501\",\n",
        "    \"--server.address=0.0.0.0\",\n",
        "    \"--server.enableCORS=false\",\n",
        "    \"--server.enableXsrfProtection=false\",\n",
        "    \"--server.headless=true\"\n",
        "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "# 3. Start the Cloudflare Tunnel\n",
        "print(\"ğŸš‡ Creating Cloudflare Tunnel...\")\n",
        "# We start the tunnel and capture its output\n",
        "process = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "# 4. Extract and Print the URL reliably\n",
        "print(\"Scanning for URL...\")\n",
        "found_url = False\n",
        "try:\n",
        "    # Read stderr because cloudflared prints the URL there\n",
        "    for line in iter(process.stderr.readline, ''):\n",
        "        if \".trycloudflare.com\" in line:\n",
        "            # Use Regex to find the URL cleanly\n",
        "            match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
        "            if match:\n",
        "                url = match.group(0)\n",
        "                print(f\"\\nğŸš€ \\033[1m OUR APP IS LIVE HERE: {url} \\033[0m\")\n",
        "                found_url = True\n",
        "                break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping...\")\n",
        "    process.terminate()\n",
        "\n",
        "if not found_url:\n",
        "    print(\"âš ï¸ Could not auto-detect URL. Please check the logs above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duNKCUKHnvJ_",
        "outputId": "fed5a78d-e412-4206-9681-0fbfef70bb4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Cloudflared...\n",
            "Starting Streamlit App...\n",
            "ğŸš‡ Creating Cloudflare Tunnel...\n",
            "Scanning for URL...\n",
            "\n",
            "ğŸš€ \u001b[1m OUR APP IS LIVE HERE: https://ver-mixing-revolution-satin.trycloudflare.com \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Demo Checklist\n",
        "\n",
        "During demo, show:\n",
        "1. Upload a manuscript image\n",
        "2. Select language mode (Chinese / Old English)\n",
        "3. Click **Digitize & Translate**\n",
        "4. Show:\n",
        "   - Original image\n",
        "   - Raw transcription output\n",
        "   - Corrected + modern translation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b-u3U6cMmArN"
      }
    }
  ]
}